where ((CURRENT_DATE-a.start_date) <={days_back_to_include} or (CURRENT_DATE-a.completion_date) <= {days_back_to_include}) and
(a.phase ilike '%2%' or a.phase ilike '%3%') and c.minimum_age_num>={minimum_age};;"
)
con <- dbConnect(drv, dbname="aact",host="aact-db.ctti-clinicaltrials.org", port=5432, user=uid, password=pwd)
oneperstudy<- dbGetQuery(con, qry1perstudy) #one per study is the one-record per table of included studies
dbDisconnect(con)
View(oneperstudy)
getwd()
setwd("C:\Users\loren\Document")
install.packages('tidytable')
vignette("share-on-a-github-website", package = "fusen")
?fusen::init_share_on_github
remotes::install_github("ThinkR-open/fusen")
vignette("share-on-a-github-website", package = "fusen")
?usethis::create_github_token
usethis::create_github_token()
gitcreds::gitcreds_set()
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% mutate(week=as.Date(week,"%m/%d/%Y"))%>% rename_columns_per_controls()
setwd('C:\\Users\\loren\\Documents\\R\\tidymmm')
librarian::shelf(tidymodels,tune,recipes,multilevelmod,tidyverse,arrow,workflowsets,rethinking,rstan)
# devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#source('tidymodels methods.R')
library(mostlytidyMMM)
source('mmm functions reducing.R')
control_file<-'example model control.xlsx'
#get control spreadsheet
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") %>% select(-desc)
tune_this_time<-get_control('tune_this_time')
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% mutate(week=as.Date(week,"%m/%d/%Y"))%>% rename_columns_per_controls()
View(var_controls)
time_id_var %>% filter(role2=='time_id')
time_id_var <-var_controls%>% filter(role2=='time_id')
time_id_var
time_id_var <-var_controls%>% filter(role=='time_id')
time_id_var
time_id_var <-var_controls%>% filter(role=='time_id') %>% select(varname) %>% unlist()
time_id_var
data1<-data1 %>% mutate(!!time_id_var = as.Date(!!time_id_var))
data1<-data1 %>% mutate(across(all_of(!!time_id_var), as.Date))
str(data1)
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()#%>% mutate(week=as.Date(week,"%m/%d/%Y"))
time_id_var <-var_controls%>% filter(role=='time_id') %>% select(varname) %>% unlist()
data1<-data1 %>% mutate(across(all_of(!!time_id_var), as.Date))
str(data1)
data1<-data1 %>% mutate(across(all_of(!!time_id_var), as.Date))
str(data1)
data1<-data1 %>% mutate(week=as.Date(week))
str(data1)
lubridate::as_date(data1$week)
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()#%>% mutate(week=as.Date(week,"%m/%d/%Y"))
lubridate::as_date(data1$week)
data1$week
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
time_id_var <-var_controls%>% filter(role=='time_id') %>% select(varname) %>% unlist()
all(is.date(data1[time_id_var]))
all(is.Date(data1[time_id_var]))
all(lubridate::is.Date(data1[time_id_var]))
lubridate::is.Date(data1[time_id_var])
data1[time_id_var]
time_id_var
data1$week
data1['week']
names(data1)
which(names(data1)==time_id_var)
lubridate::is.Date(data1[,3])
lubridate::is.Date(data1[:,3])
data1[:,3]
data1[,3]
str(data1)
lubridate::is.Date(data1$week)
lubridate::is.Date(data1[,3])
lubridate::is.Date(data1 %>% select(!!time_id_var) %>% unlist())
data1 %>% select(!!time_id_var) %>% unlist()
data1 %>% select(!!time_id_var)
hmm<-data1 %>% select(!!time_id_var) %>% unlist()
hmm[1]
as.Date(hmm[1])
as.Date(hmm[1],origin='1970-01-01')
is.numeric(data1[,which(names(data1)==time_id_var)]
)
data1[,which(names(data1)==time_id_var)]
data1[,which(names(data1)==time_id_var)]
data1[,3]
data1[,3] %>% unlist()
is.numeric(data1[,3] %>% unlist())
data1[,which(names(data1)==time_id_var)] %>% unlist()
time_id_vec<-data1[,time_id_var]
time_id_vec<-data1[,c(time_id_var)]
time_id_vec
time_id_vec<-data1[,..time_id_var]
time_id_ec
time_id_evc
time_id_vec
time_id_vec<-data1 %>% select(time_id_var) %>% unlist()
time_id_vec<-data1 %>% select(all_of(time_id_var)) %>% unlist()
time_id_vec
if(!is.numeric(time_id_vec)){stop("non numeric vector used as time_id var.
For variable set as time_id var (ie where role=time_id), please reset it to be a date or number representing a date ")}
data1$day_int=time_id_vec
data1 %>% mutate(cos1=cos(2*pi*day_int/356),
cos2=cos(4*pi*day_int/356),
cos3 =cos(6*pi*day_int/356),
cos4 = cos(8*pi*day_int/356),
cos5 = cos(10*pi*day_int/356),
sin1=sin(2*pi*day_int/356),
sin2=sin(4*pi*day_int/356),
sin3=sin(6*pi*day_int/356),
sin4=sin(8*pi*day_int/356),
sin5=sin(10*pi*day_int/356)) %>% select(-day_int))
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
time_id_var <-var_controls%>% filter(role=='time_id') %>% select(varname) %>% unlist()
time_id_vec<-data1 %>% select(all_of(time_id_var)) %>% unlist()
if(!is.numeric(time_id_vec)){stop("non numeric vector used as time_id var.
For variable set as time_id var (ie where role=time_id), please reset it to be a date or number representing a date ")}
data1$day_int=time_id_vec
data1 %>% mutate(cos1=cos(2*pi*day_int/356),
cos2=cos(4*pi*day_int/356),
cos3 =cos(6*pi*day_int/356),
cos4 = cos(8*pi*day_int/356),
cos5 = cos(10*pi*day_int/356),
sin1=sin(2*pi*day_int/356),
sin2=sin(4*pi*day_int/356),
sin3=sin(6*pi*day_int/356),
sin4=sin(8*pi*day_int/356),
sin5=sin(10*pi*day_int/356)) %>% select(-day_int)
data1<-data1 %>% mutate(cos1=cos(2*pi*day_int/356),
cos2=cos(4*pi*day_int/356),
cos3 =cos(6*pi*day_int/356),
cos4 = cos(8*pi*day_int/356),
cos5 = cos(10*pi*day_int/356),
sin1=sin(2*pi*day_int/356),
sin2=sin(4*pi*day_int/356),
sin3=sin(6*pi*day_int/356),
sin4=sin(8*pi*day_int/356),
sin5=sin(10*pi*day_int/356)) %>% select(-day_int)
add_fourier_vars<-function(data_to_use=data1,vc=var_controls){
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
time_id_vec<-data_to_use %>% select(all_of(time_id_var)) %>% unlist()
if(!is.numeric(time_id_vec)){stop("non numeric vector used as time_id var.
For variable set as time_id var (ie where role=time_id), please reset it to be a date or number representing a date ")}
data_to_use$day_int=time_id_vec
return(data_to_use %>% mutate(cos1=cos(2*pi*day_int/356),
cos2=cos(4*pi*day_int/356),
cos3 =cos(6*pi*day_int/356),
cos4 = cos(8*pi*day_int/356),
cos5 = cos(10*pi*day_int/356),
sin1=sin(2*pi*day_int/356),
sin2=sin(4*pi*day_int/356),
sin3=sin(6*pi*day_int/356),
sin4=sin(8*pi*day_int/356),
sin5=sin(10*pi*day_int/356)) %>% select(-day_int))
}
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls)
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls)
names(data1)
vc=var_controls
data_to_use=data1
#extract groups from vc
vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
}
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings)))) %>% arrange(across(all_of(groupings,time_id_var)))
}
else{
return(data_to_use %>% arrange(across(all_of(time_id_var))) ) }
}
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
groups(data1)
names(data1)
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
names(groupings)<-NULL
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings)))) %>% arrange(across(all_of(groupings,time_id_var)))
}
else{
return(data_to_use %>% arrange(across(all_of(time_id_var))) ) }
}
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
View(data1)
groups(data1)
#TODO: write function to perform checkcs on control file: 1) 1 outcome 2) role and role 2 assignment checks 3)
recipe3<-create_recipe(data_to_use = data1)#,adding_trend = get_control("add_trend"))
#build formula to match config file and dataset
built_formula<-create_formula()
#create bounds based on variable control
boundaries<-make_bound_statements()
formula_list2<-create_ulam_list(prior_controls=var_controls, model_formula=built_formula)
# tune_spec<-bayesian(family='gaussian',engine='brms',mode='regression',chains=4,iter=4000,
#                     stanvars = if(exists('inject_this_for_signs')) inject_this_for_signs else NULL,
#                     #save_model = 'mmm stan code.stan',
#                     init=0,
#                     algorithm = 'sampling',
#                     prior = if(exists('priors_to_add'))priors_to_add  else NULL)
use_these_hypers<-tune_or_fetch_hyperparameters(tune_this_time,
saved_parameter_RDS='best_hypers_lmer.RDS',
recipe_to_use=recipe3,
model_formula=built_formula,
data_set=data1,control_ranges=transform_controls)
fin_rec_3<-recipe3 %>% finalize_recipe(use_these_hypers) %>% prep()
new_data_to_fake<-bake(fin_rec_3,data1)
# data1$pred_lmer<-predict(best_mmm %>% extract_fit_engine(),new_data_to_fake)
#
# rsq(data1 %>% ungroup() ,truth = !!outcome,estimate = pred_lmer)
#now fit mmm using bayesian (sign constraint and priors and . . .)
# bayes_spec<-bayesian(family='gaussian',engine='brms',mode='regression',chains=4,iter=4000,
#                                        #stanvars = if(exists('inject_this_for_signs')) inject_this_for_signs else NULL,
#                                        save_model = 'mmm stan code.stan',
#                                        init=0,
#                                        algorithm = 'sampling',
#                                        thin=10,
#                                        # file='some_stan_model_file.RDS',
#                                        prior = if(exists('priors_to_add') )priors_to_add  else NULL)
# use_these_hypers<-readRDS('best_hypers_lmer.RDS')
# mmm_bayes_wf<-workflow() %>%  add_recipe(recipe3) %>%
#   add_model(bayes_spec,formula=as.formula(built_formula)) %>% finalize_workflow(use_these_hypers) %>% fit(data1)
# saveRDS(mmm_bayes_wf,'best_bayes_mmm.RDS')
recipe3 %>% finalize_recipe(use_these_hypers) %>% prep()-> recipe_finalized
data3<-bake(recipe_finalized ,data1)
#TODO: setup <var>_id columns for every random int!
data3 <-data3 %>% ungroup()
data3$store_id<-rethinking::coerce_index(data3$store)
data3<-data3 %>% select(-store,-product)
rethinking_results<-ulam(formula_list2,
chains=4,iter=4000,
thin=1,
data=data3,
constraints = boundaries,
sample = T,
#pars=c('b_week','a0','store_int',paste0('b_',final_predictors),'big_sigma','int_sigma'),
cmdstan = T,
file='ulam_fit_test',
cores=4,
declare_all_data=F,
messages=F
)
# model_text<-if (is.list(rethinking_results)){rethinking_results$model}else{rethinking_results@model}
# fileConn<-file("ulam stan.stan")
# writeLines(model_text, fileConn)
# close(fileConn)
data4<-data3
pp<-predict(rethinking_results,data4)
data4$hat<-pp[,1]  #colMeans(link(rethinking_results,data4)$big_model)
rsq(data4,truth = sales,estimate=hat)
ggplot(data4 ,aes(x=sales,y=hat,color=store_id))+
geom_point()+ geom_abline(slope=1,intercept=0)+ggthemes::theme_tufte()+
ggtitle("Predicted vs Actual",subtitle="store 170 looks a bit off . . .")
decomps<-get_decomps_linear()
#plot decomp
fin_pred<-get_predictors_vector(recipe3)
names(fin_pred)<-NULL
decomps<-get_decomps_linear() %>%rowwise() %>%  mutate(media=sum(c_across(all_of(fin_pred))))
decomps$week<-data1$week
decomps$sales<-data1$sales
decomps$base<-decomps$sales-decomps$media
decomps_natl<-decomps %>% group_by(week) %>% summarise(across(where(is.numeric),sum))
decomps_natl<-decomps_natl %>% pivot_longer(cols=c(-week,-sales,-media))
ggplot(data=decomps_natl,aes(x=week,y=value,fill=name)) + geom_area()+ggthemes::theme_tufte()+
ggtitle("Decomposition By Week")+
theme(legend.position = 'bottom')
################
data1$preds<-data4$hat
natl_plot<-data1  %>% group_by(week) %>% summarise(across(all_of(c('preds','sales','pred_lmer',
!!final_predictors)),
sum))
natl_plot_long<-natl_plot %>% select(week,preds,pred_lmer,sales) %>%
pivot_longer(c(preds,sales,pred_lmer),values_to='sales') %>%
mutate(name=ifelse(name=='sales','actual',name))
# ggplot(data4 %>% filter(leads<2000),aes(x=week,y=leads,group=store_id))+geom_smooth()
natl_rsq<-round(rsq(natl_plot,truth = sales,estimate=preds)[3],2)
ggplot(natl_plot_long,aes(x=week,y=sales,color=name))+geom_point() +ggthemes::theme_tufte()+
ggtitle("Aggregated Leads, LMER Predicted Leads, and Bayesian Multilevel Leads",
subtitle = paste0("rsq = ",natl_rsq))+
guides(color=  guide_legend(title=NULL))+geom_line()
#TODO:need to get accurate cost data
#media0 %>% filter(str_starts(event,"Camping World\\|\\|Facebook"),week>="2022-01-01",week<='2023-06-01') %>% summarise(sum(spend))
costs<-fread('costs_event_level_3.csv')
costs$platform<-paste0('spend_',costs$platform)
costs<-costs %>% rename(start_name=platform) %>% inner_join(var_controls %>% select(
varname,start_name)
) %>% mutate(decomp=gsub("spend_","",varname,fixed=T))
dim_event<-arrow::read_feather('dim_event.feather')
#get cost per leads for all terms with roles not time or group? already in final_predictors
media_for_summary<-read_feather('media_for_summary.feather')
compute_cost_per<-function(varname,spend_data=costs,
decomp_data=decomps){
spend_vec<-costs$spend
names(spend_vec)<-costs$decomp
decomp_total<-colSums(decomp_data)
names(decomp_total)<-names(decomp_data)
return(round(spend_vec/decomp_total,2))
}
compute_cost_per()
#changed to half cauchy sigma prior & turn off cmdstan
# saveRDS(rethinking_results,'ulam_fit.RDS')
# rethinking_results<-readRDS('ulam_out.RDS')
#manual call -- used to insert some print statments and learn that
#the ulam_list order needs to be correct for this to work
# stan_obj<-rstan::stan('ulam stan a.stan',model_name='find_the_nans',data=data3,iter=10,
#             chains=1,cores=1,algorithm='NUTS')
#example with multiple intercepts from ulam function doc
# library(rethinking)
# data(chimpanzees)
#
# d <- list(
#   pulled_left = chimpanzees$pulled_left ,
#   prosoc_left = chimpanzees$prosoc_left ,
#   condition = as.integer( 2 - chimpanzees$condition ) ,
#   actor = as.integer( chimpanzees$actor ) ,
#   blockid = as.integer( chimpanzees$block )
# )
#
# m2 <- ulam(
#   alist(
#     pulled_left ~ bernoulli(theta),
#     logit(theta) <- a + aj[actor] + bp[condition]*prosoc_left,
#     aj[actor] ~ normal( 0 , sigma_actor ),
#     a ~ normal(0,4),
#     bp[condition] ~ normal(0,1),
#     sigma_actor ~ exponential(1)
#   ) ,
#   data=d, chains=2 , cores=1 , sample=T,iter=10)
# fileConn<-file("example.stan")
# writeLines(m2@model, fileConn)
# close(fileConn)
#TODO: figure out what to do about having the tuning or not in a script -- it blocks
# some of the script flow around lmer preds . .. probably don't need that?
#TODO: reshape this as an example script
#TODO: put more of the tuning steps inside the if tune this time frame -- make a function?
#TODO: create response curves from final model results
#suitable for use with bayeisan tuning methods
setwd('C:\\Users\\loren\\Documents\\R\\tidymmm')
librarian::shelf(tidymodels,tune,recipes,multilevelmod,tidyverse,arrow,workflowsets,rethinking,rstan)
# devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#source('tidymodels methods.R')
library(mostlytidyMMM)
source('mmm functions reducing.R')
control_file<-'example model control.xlsx'
#get control spreadsheet
#var_controls -- must have 1 and only 1 role=time_id record
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") %>% select(-desc)
tune_this_time<-get_control('tune_this_time')
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
View(data1)
data1 %>% arrange(product,store,week)
data1 %>% arrange(across(all_of('product','store','week')))
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
names(groupings)<-NULL
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings)))) %>% arrange(across(all_of( c(groupings,time_id_var))))
}
else{
return(data_to_use %>% arrange(across(all_of(c(time_id_var))) )) }
}
#TODO: figure out what to do about having the tuning or not in a script -- it blocks
# some of the script flow around lmer preds . .. probably don't need that?
#TODO: reshape this as an example script
#TODO: put more of the tuning steps inside the if tune this time frame -- make a function?
#TODO: create response curves from final model results
#suitable for use with bayeisan tuning methods
setwd('C:\\Users\\loren\\Documents\\R\\tidymmm')
librarian::shelf(tidymodels,tune,recipes,multilevelmod,tidyverse,arrow,workflowsets,rethinking,rstan)
# devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#source('tidymodels methods.R')
library(mostlytidyMMM)
source('mmm functions reducing.R')
control_file<-'example model control.xlsx'
#get control spreadsheet
#var_controls -- must have 1 and only 1 role=time_id record
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") %>% select(-desc)
tune_this_time<-get_control('tune_this_time')
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
View(data1)
#TODO: figure out what to do about having the tuning or not in a script -- it blocks
# some of the script flow around lmer preds . .. probably don't need that?
#TODO: reshape this as an example script
#TODO: put more of the tuning steps inside the if tune this time frame -- make a function?
#TODO: create response curves from final model results
#suitable for use with bayeisan tuning methods
setwd('C:\\Users\\loren\\Documents\\R\\tidymmm')
librarian::shelf(tidymodels,tune,recipes,multilevelmod,tidyverse,arrow,workflowsets,rethinking,rstan)
# devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#source('tidymodels methods.R')
library(mostlytidyMMM)
source('mmm functions reducing.R')
control_file<-'example model control.xlsx'
#get control spreadsheet
#var_controls -- must have 1 and only 1 role=time_id record
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") %>% select(-desc)
tune_this_time<-get_control('tune_this_time')
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls)
View(data1)
data1<- data1%>% add_groups_and_sort(vc=var_controls)
View(data1)
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
names(groupings)<-NULL
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings)))) %>% arrange(across(all_of( c(groupings,time_id_var))))
}
else{
return(data_to_use %>% arrange(across(all_of(c(time_id_var))) )) }
}
data1<- data1%>% add_groups_and_sort(vc=var_controls)
View(data1)
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
names(groupings)<-NULL
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings)))) %>% arrange(across(all_of( c(!!groupings,!!time_id_var))))
}
else{
return(data_to_use %>% arrange(across(all_of(c(!!time_id_var))) )) }
}
data1<- data1%>% add_groups_and_sort(vc=var_controls)
View(data1)
data1 %>% arrange(all_of(c('store','product','week')))
data1 %>% arrange(across(all_of(c('store','product','week'))))
stringfor<-c('store','product','week')
data1 %>% arrange(across(all_of(stringfor)))
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
names(groupings)<-NULL
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings))) %>% arrange(across(all_of( c(!!groupings,!!time_id_var)))))
}
else{
return(data_to_use %>% arrange(across(all_of(c(!!time_id_var))) )
) }
}
#TODO: figure out what to do about having the tuning or not in a script -- it blocks
# some of the script flow around lmer preds . .. probably don't need that?
#TODO: reshape this as an example script
#TODO: put more of the tuning steps inside the if tune this time frame -- make a function?
#TODO: create response curves from final model results
#suitable for use with bayeisan tuning methods
setwd('C:\\Users\\loren\\Documents\\R\\tidymmm')
librarian::shelf(tidymodels,tune,recipes,multilevelmod,tidyverse,arrow,workflowsets,rethinking,rstan)
# devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#source('tidymodels methods.R')
library(mostlytidyMMM)
source('mmm functions reducing.R')
control_file<-'example model control.xlsx'
#get control spreadsheet
#var_controls -- must have 1 and only 1 role=time_id record
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") %>% select(-desc)
tune_this_time<-get_control('tune_this_time')
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls)
data1<- data1%>% add_groups_and_sort(vc=var_controls)
View(data1)
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
#extract groups from vc
groupings<-vc %>% filter(role2=='group') %>% select(varname) %>% unlist()
names(groupings)<-NULL
time_id_var <-vc%>% filter(role=='time_id') %>% select(varname) %>% unlist()
if(length(groupings)>0){
return(data_to_use %>% group_by(across(all_of(groupings))) %>% arrange(across(all_of( c(!!groupings,!!time_id_var)))))
}
else{
return(data_to_use %>% arrange(across(all_of(c(!!time_id_var))) )
) }
}
data1<- data1%>% add_groups_and_sort(vc=var_controls)
data1$week
View(data1)
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
View(data1)
?create_formula
