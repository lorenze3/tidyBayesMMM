# arr=X
condition_index(X).to_csv("C:\\users\\loren\\Documents\\test.csv")
diag_sum
pi_jk= ph @ diag_sum
pi_jk
exit
ph %*% diag_sum
phi_diag
reticulate::repl_python()
phi_diag
ph
pd.DataFrame(ph)
exit
ph
diag_sum
reticulate::repl_python()
diag_sum
pd.DataFrame(diag_sum)
exit
diag_sum
pi_jk=ph %*% diag_sum
pi_jk
reticulate::repl_python()
pd.DataFrame(pi_jk)
exit
pi_jk
vroom::vroom_write(pi_jk,"C:\\users\\loren\\Documents\\almostR.csv",delim=',')
vroom::vroom_write(data.frame(pi_jk),"C:\\users\\loren\\Documents\\almostR.csv",delim=',')
prop.table(ph %*% diag_sum, margin = 2)
?prop.table
reticulate::repl_python()
proportion_table = pi_jk/pi_jk.sum(axis=0)[:,None]
import numpy as np
import statsmodels.api as sm
import pandas as pd
from patsy import dmatrices
def condition_index(arr,names_arr=[]):
#if arr is a data frame, name it an np array
if isinstance(arr, pd.DataFrame):
names_arr=list(arr.columns)
arr=arr.to_numpy()
#Belsey et al urges scaling to unit column 'length' = L2 norm of each column
#It also suggests not centering the matrix unless a constant term is not estimated
#at this point, we assume the passed in array is appropriately centered (or not)
scales=np.linalg.norm(arr,axis=0)
scales=np.sqrt(np.square(arr).sum(axis=0)/arr.shape[0])
arr_scaled=arr/scales
# np.linalg.norm(arr_scaled,axis=0) -- yes, this gives all 1s!
#get the singluar value decomp, which Belsey et all start with for all calcs
U,D,Q= np.linalg.svd(arr_scaled,full_matrices=False)
#compute condition indices
#condition number is max value of D/ min value of D (of those grater than 0)
min_eig=np.where(D==0,np.inf,D).min()
#condition indices of each non-zero eigenvalue
condition_indices = D/min_eig
#variance proportion
#V is the tranpose of the output the np SVD, and is used in Belsey et al's notation
#elementwise squares of V are used for proportion of variance calcs
##TOD: which?
#V_squared=np.square(np.transpose(Q))
#V_squared=np.square(Q)
#divide each row of V_squared by appropriate squared diagonals value
#phi_kj= V_squared/np.square(D)[:,None]
#each row represents a coefficient in model /column of X -- divide each element by rowsum
# to get proportion of variance due to that coeff-singular value pair (columns are singular value)
#pi_kj = phi_kj/phi_kj.sum(axis=1)[:,None]
#directly porting over the olsrr code
V = np.transpose(Q)
phi_diag = np.diag(1 / D)
phi      = V @ phi_diag
ph       = np.transpose(np.square(phi))
diag_sum = np.diag(ph.sum(axis=1))#rowSums(ph, dims = 1))
pi_jk= ph @ diag_sum
proportion_table = pi_jk/pi_jk.sum(axis=0)[:,None]
#Blesley  et al swap indices here, so I'm going to assume that's a transpose . . .
#pi_jk=np.transpose(pi_kj) #now each column represents a coefficient/feature and each row is a condition index
#make a half - way readable output
condition_index_results=pd.DataFrame(proportion_table,columns=names_arr)
condition_index_results.insert(loc=0,column='condition_indices',value=condition_indices)
return(condition_index_results)
# arr = np.array([[1, 2, 3], [1, 5, 6],[1,0,10],[1,10,0]])
# condition_index(arr,['int','v1','v2'])
# arr=pd.DataFrame(arr,columns=["Int","V1","V2"])
# condition_index(arr,[])
#to compare to the R olsrr package for verification, we need to get the numpy array (or DF) associated
#with a linear model -- including with categorical variables expanded to columns and the intercept appended
#lets try the patsy module recommended by StatsModel  and see if we can get all that easily?
mtcars=sm.datasets.get_rdataset("mtcars")
y, X = dmatrices('mpg ~ disp + hp + wt + qsec', data=mtcars.data, return_type='dataframe')
# arr=X
condition_index(X).to_csv("C:\\users\\loren\\Documents\\test.csv")
pi_jk.sum(axis=0)[:,None]
pi_jk[:,0]
pi_jk[:,0]/pi_jk.sum(axis=0)[1]
pi_jk[:,0]
pi_jk[:,0][4]/pi_jk.sum(axis=0)[1]
pi_jk[:,0][4]/pi_jk.sum(axis=0)[0]
pi_jk[:,0]/pi_jk.sum(axis=0)[0]
pi_jk.sum(axis=0)
pi_jk.sum(axis=0)[None,:]
pi_jk/pi_jk.sum(axis=0)[None,:]
pd.DataFrame(pi_jk/pi_jk.sum(axis=0)[None,:])
import numpy as np
import statsmodels.api as sm
import pandas as pd
from patsy import dmatrices
def condition_index(arr,names_arr=[]):
#if arr is a data frame, name it an np array
if isinstance(arr, pd.DataFrame):
names_arr=list(arr.columns)
arr=arr.to_numpy()
#Belsey et al urges scaling to unit column 'length' = L2 norm of each column
#It also suggests not centering the matrix unless a constant term is not estimated
#at this point, we assume the passed in array is appropriately centered (or not)
scales=np.linalg.norm(arr,axis=0)
scales=np.sqrt(np.square(arr).sum(axis=0)/arr.shape[0])
arr_scaled=arr/scales
# np.linalg.norm(arr_scaled,axis=0) -- yes, this gives all 1s!
#get the singluar value decomp, which Belsey et all start with for all calcs
U,D,Q= np.linalg.svd(arr_scaled,full_matrices=False)
#compute condition indices
#condition number is max value of D/ min value of D (of those grater than 0)
min_eig=np.where(D==0,np.inf,D).min()
#condition indices of each non-zero eigenvalue
condition_indices = D/min_eig
#variance proportion
#V is the tranpose of the output the np SVD, and is used in Belsey et al's notation
#elementwise squares of V are used for proportion of variance calcs
##TOD: which?
#V_squared=np.square(np.transpose(Q))
#V_squared=np.square(Q)
#divide each row of V_squared by appropriate squared diagonals value
#phi_kj= V_squared/np.square(D)[:,None]
#each row represents a coefficient in model /column of X -- divide each element by rowsum
# to get proportion of variance due to that coeff-singular value pair (columns are singular value)
#pi_kj = phi_kj/phi_kj.sum(axis=1)[:,None]
#directly porting over the olsrr code
V = np.transpose(Q)
phi_diag = np.diag(1 / D)
phi      = V @ phi_diag
ph       = np.transpose(np.square(phi))
diag_sum = np.diag(ph.sum(axis=1))#rowSums(ph, dims = 1))
pi_jk= ph @ diag_sum
proportion_table = pi_jk/pi_jk.sum(axis=0)[None,:]
#Blesley  et al swap indices here, so I'm going to assume that's a transpose . . .
#pi_jk=np.transpose(pi_kj) #now each column represents a coefficient/feature and each row is a condition index
#make a half - way readable output
condition_index_results=pd.DataFrame(proportion_table,columns=names_arr)
condition_index_results.insert(loc=0,column='condition_indices',value=condition_indices)
return(condition_index_results)
# arr = np.array([[1, 2, 3], [1, 5, 6],[1,0,10],[1,10,0]])
# condition_index(arr,['int','v1','v2'])
# arr=pd.DataFrame(arr,columns=["Int","V1","V2"])
# condition_index(arr,[])
#to compare to the R olsrr package for verification, we need to get the numpy array (or DF) associated
#with a linear model -- including with categorical variables expanded to columns and the intercept appended
#lets try the patsy module recommended by StatsModel to produce it
mtcars=sm.datasets.get_rdataset("mtcars")
y, X = dmatrices('mpg ~ disp + hp + wt + qsec', data=mtcars.data, return_type='dataframe')
# arr=X
condition_index(X).to_csv("C:\\users\\loren\\Documents\\test.csv")
condition_indices.reverse()
np.reverse(condition_indices)
condition_indices[::-1]
condition_indices
import numpy as np
import statsmodels.api as sm
import pandas as pd
from patsy import dmatrices
def condition_index(arr,names_arr=[]):
#if arr is a data frame, name it an np array
if isinstance(arr, pd.DataFrame):
names_arr=list(arr.columns)
arr=arr.to_numpy()
#Belsey et al urges scaling to unit column 'length' = L2 norm of each column
#It also suggests not centering the matrix unless a constant term is not estimated
#at this point, we assume the passed in array is appropriately centered (or not)
scales=np.linalg.norm(arr,axis=0)
scales=np.sqrt(np.square(arr).sum(axis=0)/arr.shape[0])
arr_scaled=arr/scales
# np.linalg.norm(arr_scaled,axis=0) -- yes, this gives all 1s!
#get the singluar value decomp, which Belsey et all start with for all calcs
U,D,Q= np.linalg.svd(arr_scaled,full_matrices=False)
#compute condition indices
#condition number is max value of D/ min value of D (of those grater than 0)
min_eig=np.where(D==0,np.inf,D).min()
#condition indices of each non-zero eigenvalue
condition_indices = D/min_eig
#variance proportion
#V is the tranpose of the output the np SVD, and is used in Belsey et al's notation
#elementwise squares of V are used for proportion of variance calcs
##TOD: which?
#V_squared=np.square(np.transpose(Q))
#V_squared=np.square(Q)
#divide each row of V_squared by appropriate squared diagonals value
#phi_kj= V_squared/np.square(D)[:,None]
#each row represents a coefficient in model /column of X -- divide each element by rowsum
# to get proportion of variance due to that coeff-singular value pair (columns are singular value)
#pi_kj = phi_kj/phi_kj.sum(axis=1)[:,None]
#directly porting over the olsrr code
V = np.transpose(Q)
phi_diag = np.diag(1 / D)
phi      = V @ phi_diag
ph       = np.transpose(np.square(phi))
diag_sum = np.diag(ph.sum(axis=1))#rowSums(ph, dims = 1))
pi_jk= ph @ diag_sum
proportion_table = pi_jk/pi_jk.sum(axis=0)[None,:]
#Blesley  et al swap indices here, so I'm going to assume that's a transpose . . .
#pi_jk=np.transpose(pi_kj) #now each column represents a coefficient/feature and each row is a condition index
#make a half - way readable output
condition_index_results=pd.DataFrame(proportion_table,columns=names_arr)
condition_index_results.insert(loc=0,column='condition_indices',value=condition_indices[::-1])
return(condition_index_results)
# arr = np.array([[1, 2, 3], [1, 5, 6],[1,0,10],[1,10,0]])
# condition_index(arr,['int','v1','v2'])
# arr=pd.DataFrame(arr,columns=["Int","V1","V2"])
# condition_index(arr,[])
#to compare to the R olsrr package for verification, we need to get the numpy array (or DF) associated
#with a linear model -- including with categorical variables expanded to columns and the intercept appended
#lets try the patsy module recommended by StatsModel to produce it
mtcars=sm.datasets.get_rdataset("mtcars")
y, X = dmatrices('mpg ~ disp + hp + wt + qsec', data=mtcars.data, return_type='dataframe')
# arr=X
condition_index(X).to_csv("C:\\users\\loren\\Documents\\test.csv")
condition_index(X).to_csv("C:\\users\\loren\\Documents\\test.csv")
quit
vroom::vroom_write(ols_eigen_cindex(model),"c:\\users\\loren\\Documents\\testR.csv",delim=',')
library(olsrr)
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
vroom::vroom_write(ols_eigen_cindex(model),"c:\\users\\loren\\Documents\\testR.csv",delim=',')
svdxd
svdxd^2
svdxd/min(svdxd)
reticulate::repl_python()
import numpy as np
import statsmodels.api as sm
import pandas as pd
from patsy import dmatrices
def condition_index(arr,names_arr=[]):
#if arr is a data frame, name it an np array
if isinstance(arr, pd.DataFrame):
names_arr=list(arr.columns)
arr=arr.to_numpy()
#Belsey et al urges scaling to unit column 'length' = L2 norm of each column
#It also suggests not centering the matrix unless a constant term is not estimated
#at this point, we assume the passed in array is appropriately centered (or not)
scales=np.linalg.norm(arr,axis=0)
#scales=np.sqrt(np.square(arr).sum(axis=0)/arr.shape[0])
arr_scaled=arr/scales
# np.linalg.norm(arr_scaled,axis=0) -- yes, this gives all 1s!
#get the singluar value decomp, which Belsey et all start with for all calcs
U,D,Q= np.linalg.svd(arr_scaled,full_matrices=False)
#compute condition indices
#condition number is max value of D/ min value of D (of those grater than 0)
min_eig=np.where(D==0,np.inf,D).min()
#condition indices of each non-zero eigenvalue
condition_indices = D/min_eig
#variance proportion
#V is the tranpose of the output the np SVD, and is used in Belsey et al's notation
#elementwise squares of V are used for proportion of variance calcs
##TOD: which?
#V_squared=np.square(np.transpose(Q))
#V_squared=np.square(Q)
#divide each row of V_squared by appropriate squared diagonals value
#phi_kj= V_squared/np.square(D)[:,None]
#each row represents a coefficient in model /column of X -- divide each element by rowsum
# to get proportion of variance due to that coeff-singular value pair (columns are singular value)
#pi_kj = phi_kj/phi_kj.sum(axis=1)[:,None]
#directly porting over the olsrr code
V = np.transpose(Q)
phi_diag = np.diag(1 / D)
phi      = V @ phi_diag
ph       = np.transpose(np.square(phi))
diag_sum = np.diag(ph.sum(axis=1))#rowSums(ph, dims = 1))
pi_jk= ph @ diag_sum
proportion_table = pi_jk/pi_jk.sum(axis=0)[None,:]
#Blesley  et al swap indices here, so I'm going to assume that's a transpose . . .
#pi_jk=np.transpose(pi_kj) #now each column represents a coefficient/feature and each row is a condition index
#make a half - way readable output
condition_index_results=pd.DataFrame(proportion_table,columns=names_arr)
condition_index_results.insert(loc=0,column='condition_indices',value=condition_indices[::-1])
return(condition_index_results)
# arr = np.array([[1, 2, 3], [1, 5, 6],[1,0,10],[1,10,0]])
# condition_index(arr,['int','v1','v2'])
# arr=pd.DataFrame(arr,columns=["Int","V1","V2"])
# condition_index(arr,[])
#to compare to the R olsrr package for verification, we need to get the numpy array (or DF) associated
#with a linear model -- including with categorical variables expanded to columns and the intercept appended
#lets try the patsy module recommended by StatsModel to produce it
mtcars=sm.datasets.get_rdataset("mtcars")
y, X = dmatrices('mpg ~ disp + hp + wt + qsec', data=mtcars.data, return_type='dataframe')
# arr=X
condition_index(X).to_csv("C:\\users\\loren\\Documents\\test2.csv")
condition_index(X)
import numpy as np
import statsmodels.api as sm
import pandas as pd
from patsy import dmatrices
def condition_index_and_varprop(arr,names_arr=[]):
#if arr is a data frame, name it an np array
if isinstance(arr, pd.DataFrame):
names_arr=list(arr.columns)
arr=arr.to_numpy()
#Belsey et al urges scaling to unit column 'length' = L2 norm of each column
#It also suggests not centering the matrix unless a constant term is not estimated
#at this point, we assume the passed in array is appropriately centered (or not)
scales=np.linalg.norm(arr,axis=0)
#scales=np.sqrt(np.square(arr).sum(axis=0)/arr.shape[0]) -- this is the scaling used in olsrr
arr_scaled=arr/scales
# np.linalg.norm(arr_scaled,axis=0) -- yes, this gives all 1s!
#get the singluar value decomp, which Belsey et all start with for all calcs
U,D,Q= np.linalg.svd(arr_scaled,full_matrices=False)
#compute condition indices
#condition number is max value of D/ min value of D (of those grater than 0)
min_sing=np.where(D==0,np.inf,D).min()
#condition indices of each non-zero eigenvalue
#note this different from olsrr, which deviates from the text and uses sqrt(max(eig)/min(eig))
#for eig = eigenvalues of X'X matrix
condition_indices = D/min_sing
#variance proportion
#V is the tranpose of the output the np SVD, and is used in Belsey et al's notation
V = np.transpose(Q)
#elementwise squares of V are used for proportion of variance calcs
#directly porting over the olsrr code -- because I had too many transposes before
phi_diag = np.diag(1 / D)
phi      = V @ phi_diag
ph       = np.transpose(np.square(phi))
diag_sum = np.diag(ph.sum(axis=1))#rowSums(ph, dims = 1))
pi_jk= ph @ diag_sum
proportion_table = pi_jk/pi_jk.sum(axis=0)[None,:]
#make a half - way readable output
condition_index_results=pd.DataFrame(proportion_table,columns=names_arr)
condition_index_results.insert(loc=0,column='condition_indices',value=condition_indices[::-1])
return(condition_index_results)
#to compare to the R olsrr package for verification, we need to get the numpy array (or DF) associated
#with a linear model -- including with categorical variables expanded to columns and the intercept appended
#lets try the patsy module recommended by StatsModel to produce it
mtcars=sm.datasets.get_rdataset("mtcars")
y, X = dmatrices('mpg ~ disp + hp + wt + qsec', data=mtcars.data, return_type='dataframe')
# arr=X
condition_index_and_varprop(X)
reticulate::repl_python()
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from tabpfn.scripts.transformer_prediction_interface import TabPFNClassifier
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
classifier = TabPFNClassifier(device='cpu')
classifier.fit(X_train, y_train)
y_eval, p_eval = classifier.predict(X_test, return_winning_probability=True)
print('Accuracy', accuracy_score(y_test, y_eval))
from sklearn.linear_model import LogisticRegression
classifier2 = LogisticRegression()
classifer2.fit(X_train,y_train)
classifier2.fit(X_train,y_train)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
lr = LogisticRegression()
classifier2  = Pipeline([('standardize', scaler),
('log_reg', lr)])
from sklearn.pipeline import Pipeline
classifier2  = Pipeline([('standardize', scaler),
('log_reg', lr)])
classifier2.fit(X_train,y_train)
y_test_hat = classifier2.predict(X_test)
print('Accuracy',accuracy_score(y_test,y_test_hat))
install.packages(c("devtools","roxygen2","testthat",'knitr'))
?create_package()
here::dr_here()
create_package("/R/rCM360")
create_package("./R/rCM360")
# This file is intended to be a template that a user can alter to pull trial data
# from the AACT public postgresql database of clinical trials
# Written for MEDNAVIGATE, November 2022
# Author: Ted Lorenzen
# ACCT Example connections: https://aact.ctti-clinicaltrials.org/connect
#necessary libraries -- should be installed before using the script via:
#install.packages(c('RPostgreSQL','writexl','tidyverse','glue'))
library(RPostgreSQL)
library(writexl)
library(tidyverse)
library(glue)
## begin user variables
uid="lorenze3"
pwd="bJx5bhhfrsBmRyr"
## filter criteria for studies
mesh_term='migraine' #only studies that include this string (case insensitive) in the mesh terms list will be included
days_back_to_include=1826.25 #number of days back for completed_Date or start_date to include; 1826.25 is 5 years
minimum_age=18 #minimum age (in years) in study inclusion requirements  -- only those studies with a minimum_age above or equal to this value will be included
## process control
initial_output_file = 'Multi-tab Output file.xlsx' #-- name (and path, if not the working directory) of the output .xlsx file
## end user variables
#the RPostgreSQL library uses a dbi standard that requires this object (drv) to tell the
#connection that the database is PostgreSQL
drv <- dbDriver('PostgreSQL')
#what follows are 3 separate queries, one for a flat table of study information, one for all the related mesh_terms, and one for
#all outcomes for each of the included studies.
qry1perstudy<-glue("select distinct nct_id into temp these_studies from browse_conditions where mesh_term ilike '%{mesh_term}%';
select
a.nct_id,a.brief_title,a.acronym,a.created_at,a.start_date,
a.study_type,a.official_title,
a.start_date_type,a.completion_date,
a.completion_date_type,a.primary_completion_date,
a.primary_completion_date_type,
a.results_first_posted_date,a.target_duration,a.baseline_population,
a.enrollment,a.source,a.limitations_and_caveats,a.number_of_arms,
a.number_of_groups,a.why_stopped, a.biospec_description,a.has_expanded_access,
a.has_dmc,a.is_fda_regulated_device,a.is_ppsd,a.is_unapproved_device, a.last_known_status,
a.phase,a.enrollment_type,a.expanded_access_type_treatment,a.is_fda_regulated_drug,a.overall_status,
c.actual_duration,
c.minimum_age_num,c.maximum_age_num,c.number_of_primary_outcomes_to_measure,
c.number_of_secondary_outcomes_to_measure,c.number_of_other_outcomes_to_measure,
c.has_single_facility,c.has_us_facility,c.registered_in_calendar_year,c.were_results_reported,
e.population,e.gender_description,e.criteria,
e.id,e.gender,e.gender_based,e.healthy_volunteers,e.sampling_method,
i.intervention_type
from
studies a inner join
these_studies b on a.nct_id=b.nct_id
inner join calculated_values c on c.nct_id=a.nct_id
inner join eligibilities e on e.nct_id=a.nct_id
inner join interventions i on a.nct_id=i.nct_id
where ((CURRENT_DATE-a.start_date) <={days_back_to_include} or (CURRENT_DATE-a.completion_date) <= {days_back_to_include}) and
(a.phase ilike '%2%' or a.phase ilike '%3%') and c.minimum_age_num>={minimum_age};;"
)
con <- dbConnect(drv, dbname="aact",host="aact-db.ctti-clinicaltrials.org", port=5432, user=uid, password=pwd)
oneperstudy<- dbGetQuery(con, qry1perstudy) #one per study is the one-record per table of included studies
dbDisconnect(con)
View(oneperstudy)
getwd()
setwd("C:\Users\loren\Document")
install.packages('tidytable')
vignette("share-on-a-github-website", package = "fusen")
?fusen::init_share_on_github
remotes::install_github("ThinkR-open/fusen")
vignette("share-on-a-github-website", package = "fusen")
?usethis::create_github_token
usethis::create_github_token()
gitcreds::gitcreds_set()
setwd('C:\\Users\\loren\\Documents\\R\\tidymmm')
librarian::shelf(tidymodels,tune,recipes,multilevelmod,tidyverse,arrow,workflowsets,rethinking,rstan)
devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#devtools::install_local('C:\\Users\\loren\\Documents\\R\\mostlytidyMMM',force=T)
#source('tidymodels methods.R')
library(mostlytidyMMM)
source('mmm functions reducing.R')
control_file<-'example model control.xlsx'
#get control spreadsheet
#var_controls -- must have 1 and only 1 role=time_id record
#TODO: write function to perform checkcs on control file: 1) 1 outcome 2) role and role 2 assignment checks 3)
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") %>% select(-desc)
tune_this_time<-get_control('tune_this_time')
#read data and get names right;
data1<-data.table::fread("example2.csv") %>% rename_columns_per_controls()%>% mutate(week=as.Date(week,"%m/%d/%Y"))
data1<-add_fourier_vars(data_to_use=data1,vc=var_controls) %>% add_groups_and_sort(vc=var_controls)
recipe3<-create_recipe(data_to_use = data1)#,adding_trend = get_control("add_trend"))
#build formula to match config file and dataset
built_formula<-create_formula()
#create bounds based on variable control
boundaries<-make_bound_statements()
formula_list2<-create_ulam_list(prior_controls=var_controls, model_formula=built_formula)
# tune_spec<-bayesian(family='gaussian',engine='brms',mode='regression',chains=4,iter=4000,
#                     stanvars = if(exists('inject_this_for_signs')) inject_this_for_signs else NULL,
#                     #save_model = 'mmm stan code.stan',
#                     init=0,
#                     algorithm = 'sampling',
#                     prior = if(exists('priors_to_add'))priors_to_add  else NULL)
use_these_hypers<-tune_or_fetch_hyperparameters(tune_this_time,
saved_parameter_RDS='best_hypers_lmer.RDS',
recipe_to_use=recipe3,
model_formula=built_formula,
data_set=data1,control_ranges=transform_controls)
# fin_rec_3<-recipe3 %>% finalize_recipe(use_these_hypers) %>% prep()
# new_data_to_fake<-bake(fin_rec_3,data1)
# data1$pred_lmer<-predict(best_mmm %>% extract_fit_engine(),new_data_to_fake)
#
# rsq(data1 %>% ungroup() ,truth = !!outcome,estimate = pred_lmer)
#now fit mmm using bayesian (sign constraint and priors and . . .)
# bayes_spec<-bayesian(family='gaussian',engine='brms',mode='regression',chains=4,iter=4000,
#                                        #stanvars = if(exists('inject_this_for_signs')) inject_this_for_signs else NULL,
#                                        save_model = 'mmm stan code.stan',
#                                        init=0,
#                                        algorithm = 'sampling',
#                                        thin=10,
#                                        # file='some_stan_model_file.RDS',
#                                        prior = if(exists('priors_to_add') )priors_to_add  else NULL)
# use_these_hypers<-readRDS('best_hypers_lmer.RDS')
# mmm_bayes_wf<-workflow() %>%  add_recipe(recipe3) %>%
#   add_model(bayes_spec,formula=as.formula(built_formula)) %>% finalize_workflow(use_these_hypers) %>% fit(data1)
# saveRDS(mmm_bayes_wf,'best_bayes_mmm.RDS')
recipe3 %>% finalize_recipe(use_these_hypers) %>% prep()-> recipe_finalized
data3<-bake(recipe_finalized ,data1)
#TODO: setup <var>_id columns for every random int!
data3 <-data3 %>% ungroup()
data3$store_id<-rethinking::coerce_index(data3$store)
data3<-data3 %>% select(-store,-product)
rethinking_results<-ulam(formula_list2,
chains=4,iter=4000,
thin=1,
data=data3,
constraints = boundaries,
sample = T,
#pars=c('b_week','a0','store_int',paste0('b_',final_predictors),'big_sigma','int_sigma'),
cmdstan = T,
file='ulam_fit_test_rs',
cores=4,
declare_all_data=F,
messages=F
)
